{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import glob\n",
    "from lxml.html import parse\n",
    "from sphfile import SPHFile\n",
    "import pydub\n",
    "import audiosegment\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "sr = 16000\n",
    "dropout = 0.3\n",
    "half = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 2\n",
    "torch.cuda.set_device(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class OverlayDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv, compute_original = False):\n",
    "        super().__init__()\n",
    "        self.overlays = pd.read_csv(csv)\n",
    "        self.speakers = list(set(self.overlays['first_speaker']).union(set(self.overlays['second_speaker'])))\n",
    "        self.spkr2idx = {spkr:i for i, spkr in enumerate(self.speakers)}\n",
    "        self.compute_original = compute_original\n",
    "    def __len__(self):\n",
    "        return len(self.overlays)\n",
    "    def __getitem__(self, idx):\n",
    "        overlay = self.overlays.iloc[idx]\n",
    "        first_segment = np.load(overlay['first_file'])/(2**15)\n",
    "        second_segment = np.load(overlay['second_file'])/(2**15)\n",
    "        #padding to compensate rounding errors\n",
    "        if len(first_segment)>len(second_segment):\n",
    "            padding = np.zeros(len(first_segment)-len(second_segment))\n",
    "            second_segment = np.concatenate((second_segment, padding))\n",
    "        \n",
    "        if len(first_segment)<len(second_segment):\n",
    "            padding = np.zeros(len(second_segment)-len(first_segment))\n",
    "            first_segment = np.concatenate((first_segment, padding))\n",
    "        \n",
    "        \n",
    "        first_idx  = self.spkr2idx[overlay['first_speaker']]\n",
    "        second_idx = self.spkr2idx[overlay['second_speaker']]\n",
    "        target = np.zeros(len(self.speakers))\n",
    "        target[first_idx] = 1.0\n",
    "        target[second_idx] = 1.0\n",
    "        if self.compute_original:\n",
    "            return self.make_spectrogram(first_segment), self.make_spectrogram(second_segment),\\\n",
    "                self.make_spectrogram(first_segment+second_segment), target\n",
    "        else:\n",
    "            return self.make_spectrogram(first_segment+second_segment), target\n",
    "    def make_spectrogram(self, segment):\n",
    "        segment = segment[50:-50] # make size 200\n",
    "        S = librosa.feature.mfcc(segment, sr=16000, n_mfcc=20, dct_type=2, n_fft = 1024, hop_length = 160)[1:14].T\n",
    "        # 200*13\n",
    "        S1 = np.diff(S)\n",
    "        S2 = np.diff(S1)\n",
    "        S = np.concatenate((S, S1, S2), axis = -1)\n",
    "        return S\n",
    "    \n",
    "trainset = OverlayDataSet('overlay-train.csv', False)\n",
    "valset = OverlayDataSet('overlay-val.csv', False)\n",
    "testset = OverlayDataSet('overlay-test.csv', False)\n",
    "\n",
    "spec3, target = trainset[0]\n",
    "plt.figure(figsize = (20, 6))\n",
    "if trainset.compute_original:\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(spec1[0].T)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(spec2[0].T)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(spec3[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe try drastically increasing channel number in residual attention stage to see if it overfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bestacc:', 0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads_2 = 4 # MHA heads\n",
    "\n",
    "\n",
    "class Baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn = nn.LayerNorm(36)\n",
    "        self.reshape =  Lambda(lambda x: x.permute((1, 0, 2))) # L * batch_size * (num_heads*128)\n",
    "        self.lstm = nn.LSTM(36, 32, 2, batch_first = False, bidirectional = True, dropout = dropout) # L * batch_size * 200 * n_hidden\n",
    "        self.mha =  torch.nn.MultiheadAttention(64, num_heads = num_heads_2, dropout=dropout, bias=True, kdim=64, vdim=64) # L * N * 64\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.average = Lambda(lambda x: x.mean(dim = 0)) # batch * n_hidden\n",
    "        self.tanh = nn.Tanh()\n",
    "        #self.norm = Lambda(lambda x: torch.nn.functional.normalize(x, p = 2, dim = 1)) # L2 normalize across n_hidden\n",
    "        self.fc2 = nn.Linear(32, 20)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, X):\n",
    "        X = self.bn(X)\n",
    "        X = self.reshape(X)\n",
    "        X, _ = self.lstm(X)\n",
    "        X, _ = self.mha(X, X, X)\n",
    "        X = self.fc1(X)\n",
    "        X = self.average(X)\n",
    "        X = self.tanh(X)\n",
    "        X = self.fc2(X)\n",
    "        X = self.sigmoid(X)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "overnet = Baseline().cuda(device)\n",
    "    \n",
    "# tune hidden layers smaller if overfit\n",
    "optimizer = torch.optim.Adam(overnet.parameters(), 0.001)\n",
    "\n",
    "if os.path.exists('models/baseline.pth'):\n",
    "    print('load model')\n",
    "    checkpoint = torch.load('models/baseline.pth')\n",
    "    overnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "    try:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    except:\n",
    "        print('cannot load optimizer')\n",
    "    loss = checkpoint['loss']\n",
    "    if 'bestacc' in checkpoint:\n",
    "        bestacc = checkpoint['bestacc']\n",
    "    else:\n",
    "        bestacc = 0.0\n",
    "else:\n",
    "    bestacc = 0.0\n",
    "    \n",
    "if half:\n",
    "    overnet.half()  # convert to half precision\n",
    "    for layer in overnet.modules():\n",
    "        if isinstance(layer, nn.BatchNorm2d):\n",
    "            layer.float()\n",
    "            \n",
    "overnet.train()\n",
    "'bestacc:', bestacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also Do metrics on hitting a single person right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87dc06949cda4726b73f36363931b480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1378.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def find_max2(tensor):\n",
    "    array = tensor.cpu().detach().numpy()\n",
    "    max2 = []\n",
    "    for row in array:\n",
    "        max2.append(np.argsort(row)[::-1][:2])\n",
    "    return np.array(max2)\n",
    "\n",
    "def compute_corrects(tensor1, tensor2):\n",
    "    max_1, max_2 = find_max2(tensor1), find_max2(tensor2)\n",
    "    batch_size = max_1.shape[0]\n",
    "    corrects = 0\n",
    "    for i in range(batch_size):\n",
    "        if Counter(max_1[i])==Counter(max_2[i]):\n",
    "            corrects+=1\n",
    "    return corrects\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, pin_memory = True, num_workers = 16)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True, pin_memory = True, num_workers = 16)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "for epoch in range(64):\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    for batch_idx, (spec, target) in enumerate(tqdm(trainloader)):\n",
    "        optimizer.zero_grad()\n",
    "        spec, target = spec.float(), target.float()\n",
    "        if half:\n",
    "            spec, target = spec.half(),target.half()\n",
    "        spec = spec.cuda(device)\n",
    "        target = target.cuda(device)\n",
    "\n",
    "        out = overnet(spec)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(overnet.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "                \n",
    "        running_loss += loss.item()\n",
    "        running_accuracy += compute_corrects(out, target)/64\n",
    "        if batch_idx % 200 == 199:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f accuracy: %.3f' % \n",
    "                  (epoch + 1, batch_idx + 1, running_loss / 200, running_accuracy / 200))\n",
    "            running_loss = 0.0\n",
    "            running_accuracy = 0.0\n",
    "            torch.save({\n",
    "            'model_state_dict': overnet.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "            }, 'models/baseline.pth')\n",
    "        #measure time\n",
    "        #print('batch time: ', str(time.time()-lasttime)[:4])\n",
    "        lasttime = time.time()\n",
    "        \n",
    "        \n",
    "    corrects = 0\n",
    "    for batch_idx, (spec, target) in enumerate(tqdm(valloader)):\n",
    "        spec, target = spec.float(), target.float()\n",
    "        if half:\n",
    "            spec, target = spec.half(), target.half()\n",
    "        spec = spec.cuda(device)\n",
    "        target = target.cuda(device)\n",
    "        overnet.eval()\n",
    "        out = overnet(spec) \n",
    "        corrects += compute_corrects(out, target)\n",
    "    print('val acc:', corrects/len(valset))\n",
    "    if corrects/len(valset) > bestacc:\n",
    "        bestacc = corrects/len(valset)\n",
    "        torch.save({\n",
    "        'model_state_dict': overnet.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'bestacc': bestacc\n",
    "        }, 'models/best-baseline.pth')\n",
    "    overnet.train()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
