{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import glob\n",
    "from lxml.html import parse\n",
    "from sphfile import SPHFile\n",
    "import pydub\n",
    "import audiosegment\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "sr = 16000\n",
    "dropout = 0.3\n",
    "half = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 3\n",
    "torch.cuda.set_device(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class OverlayDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv, compute_original = False):\n",
    "        super().__init__()\n",
    "        self.overlays = pd.read_csv(csv)\n",
    "        self.speakers = list(set(self.overlays['first_speaker']).union(set(self.overlays['second_speaker'])))\n",
    "        self.spkr2idx = {spkr:i for i, spkr in enumerate(self.speakers)}\n",
    "        self.compute_original = compute_original\n",
    "    def __len__(self):\n",
    "        return len(self.overlays)\n",
    "    def __getitem__(self, idx):\n",
    "        overlay = self.overlays.iloc[idx]\n",
    "        first_segment = np.load(overlay['first_file'])/(2**15)\n",
    "        second_segment = np.load(overlay['second_file'])/(2**15)\n",
    "        #padding to compensate rounding errors\n",
    "        if len(first_segment)>len(second_segment):\n",
    "            padding = np.zeros(len(first_segment)-len(second_segment))\n",
    "            second_segment = np.concatenate((second_segment, padding))\n",
    "        \n",
    "        if len(first_segment)<len(second_segment):\n",
    "            padding = np.zeros(len(second_segment)-len(first_segment))\n",
    "            first_segment = np.concatenate((first_segment, padding))\n",
    "        \n",
    "        \n",
    "        first_idx  = self.spkr2idx[overlay['first_speaker']]\n",
    "        second_idx = self.spkr2idx[overlay['second_speaker']]\n",
    "        target = np.zeros(len(self.speakers))\n",
    "        target[first_idx] = 1.0\n",
    "        target[second_idx] = 1.0\n",
    "        if self.compute_original:\n",
    "            return self.make_spectrogram(first_segment), self.make_spectrogram(second_segment),\\\n",
    "                self.make_spectrogram(first_segment+second_segment), target\n",
    "        else:\n",
    "            return self.make_spectrogram(first_segment+second_segment), target\n",
    "    def make_spectrogram(self, segment):\n",
    "        segment = segment[50:-50] # make size 200\n",
    "        S = librosa.feature.melspectrogram(segment, n_mels = 256, n_fft = 1024, hop_length = 160) # 32 ms window, 10 ms hop\n",
    "        S_dB = librosa.power_to_db(S, ref=np.max).T[None, :, :] # add channel dimension\n",
    "        S_dB = (S_dB+40)/40\n",
    "        return(S_dB)\n",
    "trainset = OverlayDataSet('overlay-train.csv', False)\n",
    "valset = OverlayDataSet('overlay-val.csv', False)\n",
    "spec3, target = trainset[0]\n",
    "plt.figure(figsize = (20, 6))\n",
    "if trainset.compute_original:\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(spec1[0].T)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(spec2[0].T)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(spec3[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe try drastically increasing channel number in residual attention stage to see if it overfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bestacc:', 0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import functools\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.stride = stride\n",
    "        self.bn1 = nn.BatchNorm2d(input_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(input_channels, output_channels, 1, 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(output_channels, output_channels, 3, stride, padding = 1, bias = False)\n",
    "        self.bn3 = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(output_channels, output_channels, 1, 1, bias = False)\n",
    "        self.conv4 = nn.Conv2d(input_channels, output_channels , 1, stride, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.bn1(x)\n",
    "        out1 = self.relu(out)\n",
    "        out = self.conv1(out1)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        if (self.input_channels != self.output_channels) or (self.stride !=1 ):\n",
    "            residual = self.conv4(out1)\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "class AttentionModule_stage1(nn.Module):\n",
    "    # input size is 56*56\n",
    "    def __init__(self, in_channels, out_channels, size1=(200, 128), size2=(100, 64), size3=(50, 32)):\n",
    "        super(AttentionModule_stage1, self).__init__()\n",
    "        self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.trunk_branches = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "         )\n",
    "\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.softmax1_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.softmax2_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.skip2_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.mpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.softmax3_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "        self.interpolation3 = nn.UpsamplingBilinear2d(size=size3)\n",
    "\n",
    "        self.softmax4_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.interpolation2 = nn.UpsamplingBilinear2d(size=size2)\n",
    "\n",
    "        self.softmax5_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n",
    "\n",
    "        self.softmax6_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels , kernel_size = 1, stride = 1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels , kernel_size = 1, stride = 1, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #batch_size, nheads, length, n_mels = x.shape\n",
    "        x = self.first_residual_blocks(x)\n",
    "        out_trunk = self.trunk_branches(x)\n",
    "        out_mpool1 = self.mpool1(x) # 100x64\n",
    "        out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "        out_skip1_connection = self.skip1_connection_residual_block(out_softmax1)\n",
    "        out_mpool2 = self.mpool2(out_softmax1) # 50x32\n",
    "        out_softmax2 = self.softmax2_blocks(out_mpool2)\n",
    "        out_skip2_connection = self.skip2_connection_residual_block(out_softmax2)\n",
    "        out_mpool3 = self.mpool3(out_softmax2) # 25x16\n",
    "        out_softmax3 = self.softmax3_blocks(out_mpool3) \n",
    "        out_interp3 = self.interpolation3(out_softmax3) + out_softmax2\n",
    "        out = out_interp3 + out_skip2_connection\n",
    "        out_softmax4 = self.softmax4_blocks(out)\n",
    "        out_interp2 = self.interpolation2(out_softmax4) + out_softmax1\n",
    "        out = out_interp2 + out_skip1_connection\n",
    "        out_softmax5 = self.softmax5_blocks(out)\n",
    "        out_interp1 = self.interpolation1(out_softmax5) + out_trunk\n",
    "        out_softmax6 = self.softmax6_blocks(out_interp1)\n",
    "        out = (1 + out_softmax6) * out_trunk\n",
    "        out_last = self.last_blocks(out)\n",
    "        return out_last\n",
    "\n",
    "num_heads = 8 # Residual Attention Channels\n",
    "num_heads_2 = 4 # MHA heads\n",
    "\n",
    "\n",
    "class OverlayNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.downsample1 = ResidualBlock(1, num_heads, (1, 2))\n",
    "        self.res_att = AttentionModule_stage1(num_heads, num_heads)  # batch_size * num_heads * L *128\n",
    "        self.downsample2 = nn.Sequential(ResidualBlock(num_heads, num_heads//2),\n",
    "                                        ResidualBlock(num_heads//2, 2),\n",
    "                                        ResidualBlock(2, 2, (1, 2)))\n",
    "        self.reshape =  Lambda(lambda x: x.permute((1, 2, 0, 3))) # L * batch_size * (num_heads*128)\n",
    "        self.lstm = nn.LSTM(64, 32, 2, batch_first = False, bidirectional = True, dropout = dropout) # L * batch_size * 200 * n_hidden\n",
    "        self.mha =  torch.nn.MultiheadAttention(64, num_heads = num_heads_2, dropout=dropout, bias=True, kdim=64, vdim=64) # L * N * 64\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.average = Lambda(lambda x: x.mean(dim = 0)) # batch * n_hidden\n",
    "        self.tanh = nn.Tanh()\n",
    "        #self.norm = Lambda(lambda x: torch.nn.functional.normalize(x, p = 2, dim = 1)) # L2 normalize across n_hidden\n",
    "        self.fc2 = nn.Linear(32, 20)\n",
    "        self.softmax = nn.Softmax(1)\n",
    "    def forward(self, X):\n",
    "        X = self.downsample1(X)\n",
    "        #print('first downsample ', X.shape)\n",
    "        X = self.res_att(X)\n",
    "        #print('residual attention ', X.shape)\n",
    "        X = self.downsample2(X)\n",
    "        #print('second downsample ', X.shape)\n",
    "        X = self.reshape(X)\n",
    "        X1, X2 = X[0], X[1]\n",
    "        X1,_ = self.lstm(X1)\n",
    "        X2,_ = self.lstm(X1)\n",
    "        #print('lstm ', X.shape)\n",
    "        X1,_ = self.mha(X1, X1, X1)\n",
    "        X2,_ = self.mha(X2, X2, X2)\n",
    "        #print('mha ', X.shape)\n",
    "        X1 = self.fc1(X1)\n",
    "        X2 = self.fc1(X2)\n",
    "        #print('dense ', X.shape)\n",
    "        X1 = self.average(X1)\n",
    "        X2 = self.average(X2)\n",
    "        #print('mean ', X.shape)\n",
    "        X1 = self.tanh(X1)\n",
    "        X2 = self.tanh(X2)\n",
    "        X1 = self.fc2(X1)\n",
    "        X2 = self.fc2(X2)\n",
    "        X1 = self.softmax(X1)\n",
    "        X2 = self.softmax(X2)\n",
    "        X = torch.stack([X1,X2], dim=0)\n",
    "        X,_ = torch.max(X, dim=0)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "overnet = OverlayNet().cuda(device)\n",
    "    \n",
    "# tune hidden layers smaller if overfit\n",
    "optimizer = torch.optim.Adam(overnet.parameters(), 0.001)\n",
    "\n",
    "if os.path.exists('models/overnet8.pth'):\n",
    "    print('load model')\n",
    "    checkpoint = torch.load('models/overnet8.pth')\n",
    "    overnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "    try:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    except:\n",
    "        print('cannot load optimizer')\n",
    "    loss = checkpoint['loss']\n",
    "    if 'bestacc' in checkpoint:\n",
    "        bestacc = checkpoint['bestacc']\n",
    "    else:\n",
    "        bestacc = 0.0\n",
    "else:\n",
    "    bestacc = 0.0\n",
    "    \n",
    "if half:\n",
    "    overnet.half()  # convert to half precision\n",
    "    for layer in overnet.modules():\n",
    "        if isinstance(layer, nn.BatchNorm2d):\n",
    "            layer.float()\n",
    "            \n",
    "overnet.train()\n",
    "'bestacc:', bestacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also Do metrics on hitting a single person right\n",
    "## Theoretical justification that when reducing number of channels, neural network could learn to pair similar spectrum representations with each other: only pitch lines that are greater than a certain threshold will pass relu and become postivie. Batchnorm makes this effect stronger. Therefore, when two channels represent spectrums of different people, when they are added the output will be nothing if it doesn't pass the relu threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31b3b8470f5418cb266d834b1fbbf54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1378.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.307 accuracy: 0.023\n",
      "[1,   400] loss: 0.263 accuracy: 0.069\n",
      "[1,   600] loss: 0.239 accuracy: 0.112\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "def find_max2(tensor):\n",
    "    array = tensor.cpu().detach().numpy()\n",
    "    max2 = []\n",
    "    for row in array:\n",
    "        max2.append(np.argsort(row)[::-1][:2])\n",
    "    return np.array(max2)\n",
    "\n",
    "def compute_corrects(tensor1, tensor2):\n",
    "    max_1, max_2 = find_max2(tensor1), find_max2(tensor2)\n",
    "    batch_size = max_1.shape[0]\n",
    "    corrects = 0\n",
    "    for i in range(batch_size):\n",
    "        if Counter(max_1[i])==Counter(max_2[i]):\n",
    "            corrects+=1\n",
    "    return corrects\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory = True, num_workers = 16)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True, pin_memory = True, num_workers = 16)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "for epoch in range(64):\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    for batch_idx, (spec, target) in enumerate(tqdm(trainloader)):\n",
    "        optimizer.zero_grad()\n",
    "        spec, target = spec.float(), target.float()\n",
    "        if half:\n",
    "            spec, target = spec.half(),target.half()\n",
    "        spec = spec.cuda(device)\n",
    "        target = target.cuda(device)\n",
    "\n",
    "        out = overnet(spec)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(overnet.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "                \n",
    "        running_loss += loss.item()\n",
    "        running_accuracy += compute_corrects(out, target)/batch_size\n",
    "        if batch_idx % 200 == 199:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f accuracy: %.3f' % \n",
    "                  (epoch + 1, batch_idx + 1, running_loss / 200, running_accuracy / 200))\n",
    "            running_loss = 0.0\n",
    "            running_accuracy = 0.0\n",
    "            torch.save({\n",
    "            'model_state_dict': overnet.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "            }, 'models/overnet8.pth')\n",
    "        #measure time\n",
    "        #print('batch time: ', str(time.time()-lasttime)[:4])\n",
    "        lasttime = time.time()\n",
    "        \n",
    "        \n",
    "    corrects = 0\n",
    "    for batch_idx, (spec, target) in enumerate(tqdm(valloader)):\n",
    "        spec, target = spec.float(), target.float()\n",
    "        if half:\n",
    "            spec, target = spec.half(), target.half()\n",
    "        spec = spec.cuda(device)\n",
    "        target = target.cuda(device)\n",
    "        overnet.eval()\n",
    "        out = overnet(spec) \n",
    "        corrects += compute_corrects(out, target)\n",
    "    print('val acc:', corrects/len(valset))\n",
    "    if corrects/len(valset) > bestacc:\n",
    "        bestacc = corrects/len(valset)\n",
    "        torch.save({\n",
    "        'model_state_dict': overnet.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'bestacc': bestacc\n",
    "        }, 'models/best-overnet8.pth')\n",
    "    overnet.train()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
